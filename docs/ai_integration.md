# توثيق تكامل الذكاء الاصطناعي في منصة السعادة ERP

**المسار الرسمي للمشروع:**  
D:\Alsaada-ERP\Alsaada_ERP_Platform

---

## 1. الغرض من التكامل

يهدف تكامل الذكاء الاصطناعي في منصة السعادة ERP إلى:
- تقديم حلول واقتراحات تلقائية عند ظهور المشاكل البرمجية أو التشغيلية.
- إتاحة مساعد ذكي داخل النظام للرد على استفسارات المستخدمين وتبسيط العمليات.
- دعم تلخيص التقارير والبيانات الطويلة بشكل ذكي وفوري.
- تحليل الطلبات النصية للمستخدم (NLP) وتحويلها إلى إجراءات مباشرة داخل النظام.

---

## 2. التقنيات المستخدمة

- **llama.cpp**: تشغيل نموذج لغة كبيرة (LLM) محليًا على الجهاز بدون الحاجة لاتصال إنترنت.
- **Python**: وسيط للتواصل مع llama.cpp (إرسال prompts واستقبال الردود).
- **PyWebview**: ربط بين الواجهة (HTML/JS) وخدمات الذكاء الاصطناعي عبر بايثون.
- **نموذج Llama 3 أو ما شابه (gguf)**: ملف النموذج المستعمل فعليًا.

---

## 3. سيناريوهات الاستخدام

### أ. اقتراح حل للمشاكل البرمجية أو التشغيلية
- عند تسجيل أو ظهور خطأ، يظهر زر "اقتراح حل" بجواره.
- عند الضغط، يتم إرسال وصف الخطأ تلقائيًا إلى خدمة الذكاء الاصطناعي.
- يعرض النظام ردًا مبسطًا وعمليًا (اقتراح حل أو شرح مختصر للسبب).

### ب. مساعد ذكي للمستخدم
- زر أو تبويب خاص في النظام يتيح للمستخدم كتابة سؤال أو طلب مساعدة.
- يستلم المساعد الطلب، يجيب عليه بالعربية ويقدم الخطوات أو التوجيهات.

### ج. تلخيص التقارير الطويلة
- عند عرض تقرير كبير أو معقد، يمكن للمستخدم الضغط على "تلخيص التقرير".
- الذكاء الاصطناعي يستخرج أهم النقاط ويعرضها بشكل مختصر.

---

## 4. تنظيم الملفات البرمجية المتعلقة بالتكامل

| الملف                                         | الوصف                                                              |
|-----------------------------------------------|--------------------------------------------------------------------|
| app/core/llama_client.py                      | وحدة وسيطة للتخاطب مع خدمة الذكاء الاصطناعي (llama.cpp)            |
| app/modules/ai_assistant/controller.py        | منطق معالجة الطلبات من الواجهة وربطها مع llama_client              |
| web/templates/ai_assistant.html               | واجهة المستخدم للمساعد الذكي                                       |
| web/static/js/ai_assistant.js                 | جافاسكريبت لإدارة تفاعل المستخدم مع المساعد الذكي                  |
| llama/models/                                 | ملفات النماذج (gguf)                                               |
| llama/run_llama.bat                           | سكريبت تشغيل نموذج الذكاء الاصطناعي كخدمة أو عملية مستقلة          |

---

## 5. خطوات التشغيل والتكامل

1. **تشغيل خدمة llama.cpp**  
   - تحميل النموذج المناسب (مثلاً Llama 3 8B gguf).
   - تشغيل الخدمة عبر سكريبت run_llama.bat أو أمر مخصص (توضيح في README).

2. **التواصل من بايثون مع النموذج**  
   - `llama_client.py` يستقبل النص المطلوب (خطأ أو سؤال)، يرسل للنموذج، يستقبل الرد ويعيده للواجهة.

3. **واجهة المستخدم**  
   - زر أو تبويب خاص في النظام لتفعيل المساعد الذكي أو طلب تلخيص أو اقتراح حل.
   - النتائج تظهر مباشرة في الواجهة (HTML/JS).

4. **توثيق كل تفاعل**  
   - كل استخدام للذكاء الاصطناعي يسجل في سجل التطوير (development_log.md) وأحيانًا في logs/ للاختبار والتحليل.

---

## 6. أمثلة على Prompts ودوال الاستخدام

### مثال Prompt لاقتراح حل:
